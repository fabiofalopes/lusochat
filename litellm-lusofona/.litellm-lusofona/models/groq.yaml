# groq.yaml
# âš¡ Groq Model Configurations
# Information verified and updated based on official Groq documentation as of September 23, 2025.
# 
# This configuration includes:
# - LLM Models (Language Models)
# - STT Models (Speech-to-Text / Audio Transcription)
# - TTS Models (Text-to-Speech)
# - Content Moderation & Safety Models
#
# PRICING NOTES:
# - Using Free Tier but including official pricing for consumption tracking
# - All pricing is per 1M tokens unless specified otherwise
# - Audio models: Whisper V3 Large ($0.111/hour), Whisper V3 Turbo ($0.04/hour)
# - TTS models: PlayAI Dialog ($50.00 per 1M characters)
# - Compound systems: Pass-through pricing to underlying models + tool costs

model_list:
  # Groq - Llama 3.1 8B Instant
  - model_name: Groq-Llama-3.1-8B-Instant
    litellm_params:
      model: groq/llama-3.1-8b-instant
      api_base: https://api.groq.com/openai/v1
      api_key: os.environ/GROQ_API_KEY
      rpm: 30                    # Free Tier: 30 RPM, 14.4K RPD
      tpm: 6000                  # Free Tier: 6K TPM, 500K TPD
      # max_tokens: 128000         # Maximum context window
      input_cost_per_token: 0.00000005  # $0.05 per 1M input tokens
      output_cost_per_token: 0.00000008 # $0.08 per 1M output tokens
    model_info:
      mode: chat
      max_tokens: 128000

  # Groq - Llama 4 Scout 17B 16E Instruct
  - model_name: Groq-Llama-4-Scout-17B-16E-Instruct
    litellm_params:
      model: groq/meta-llama/llama-4-scout-17b-16e-instruct
      api_base: https://api.groq.com/openai/v1
      api_key: os.environ/GROQ_API_KEY
      rpm: 30                    # Free Tier: 30 RPM, 1K RPD
      tpm: 30000                 # Free Tier: 30K TPM, 500K TPD
      # max_tokens: 128000
      input_cost_per_token: 0.00000011  # $0.11 per 1M input tokens
      output_cost_per_token: 0.00000034 # $0.34 per 1M output tokens
    model_info:
      mode: chat
      max_tokens: 128000

  # Groq - Qwen3 32B (Recommended replacement for deprecated mistral-saba-24b)
    # Groq - Qwen3 2.5 32B Instruct Preview (UQFF v001)
  - model_name: Groq-Qwen3-2.5-32B-Instruct-Preview
    litellm_params:
      model: groq/qwen/qwen2.5-32b-instruct-uqff-v001
      api_base: https://api.groq.com/openai/v1
      api_key: os.environ/GROQ_API_KEY
      rpm: 60                    # Free Tier: 60 RPM, 1K RPD (qwen/qwen3-32b)
      tpm: 6000                  # Free Tier: 6K TPM, 500K TPD
      input_cost_per_token: 0.00000011  # $0.11 per 1M input tokens
      output_cost_per_token: 0.00000033 # $0.33 per 1M output tokens
    model_info:
      mode: chat
      max_tokens: 131072

  # Groq - Llama 3.3 70B Versatile
    # Groq - Llama 3.3 70B Instruct Preview Speculative
  - model_name: Groq-Llama-3.3-70B-Instruct-Preview-Spec
    litellm_params:
      model: groq/meta-llama/llama-3.3-70b-instruct-preview-speculative
      api_base: https://api.groq.com/openai/v1
      api_key: os.environ/GROQ_API_KEY
      rpm: 30                    # Free Tier: 30 RPM, 1K RPD (llama-3.3-70b-versatile)
      tpm: 12000                 # Free Tier: 12K TPM, 100K TPD
      input_cost_per_token: 0.00000059  # $0.59 per 1M input tokens
      output_cost_per_token: 0.00000079 # $0.79 per 1M output tokens
    model_info:
      mode: chat
      max_tokens: 131072

  # Groq - Llama 4 Maverick 17B 128E Instruct
  - model_name: Groq-Llama-4-Maverick-17B-128E-Instruct
    litellm_params:
      model: groq/meta-llama/llama-4-maverick-17b-128e-instruct
      api_base: https://api.groq.com/openai/v1
      api_key: os.environ/GROQ_API_KEY
      rpm: 30                    # Free Tier: 30 RPM, 1K RPD
      tpm: 6000                  # Free Tier: 6K TPM, 500K TPD
      # max_tokens: 128000
      input_cost_per_token: 0.0000002   # $0.20 per 1M input tokens
      output_cost_per_token: 0.0000006  # $0.60 per 1M output tokens
    model_info:
      mode: chat
      max_tokens: 128000

  # Groq - Moonshot Kimi K2 0905 Instruct (Updated replacement for deprecated kimi-k2-instruct)
  - model_name: Groq-Moonshot-Kimi-K2-0905-Instruct
    litellm_params:
      model: groq/moonshotai/kimi-k2-instruct-0905
      api_base: https://api.groq.com/openai/v1
      api_key: os.environ/GROQ_API_KEY
      rpm: 60                    # Free Tier: 60 RPM, 1K RPD (both kimi variants)
      tpm: 10000                 # Free Tier: 10K TPM, 300K TPD
      # max_tokens: 256000          # Updated: 256K context window
      input_cost_per_token: 0.000001    # $1.00 per 1M input tokens
      output_cost_per_token: 0.000003   # $3.00 per 1M output tokens
    model_info:
      mode: chat
      max_tokens: 256000

  # ============================================================================
  # CONTENT MODERATION & SAFETY MODELS
  # ============================================================================

  # Groq - Llama Guard 4 12B (Production)
  - model_name: Groq-Llama-Guard-4-12B
    litellm_params:
      model: groq/meta-llama/llama-guard-4-12b
      api_base: https://api.groq.com/openai/v1
      api_key: os.environ/GROQ_API_KEY
      rpm: 30                    # Free Tier: 30 RPM, 14.4K RPD
      tpm: 15000                 # Free Tier: 15K TPM, 500K TPD
      # max_tokens: 1024           # Specialized for safety classification
      input_cost_per_token: 0.00000020  # $0.20 per 1M input tokens
      output_cost_per_token: 0.00000020 # $0.20 per 1M output tokens
    model_info:
      mode: chat
      max_tokens: 131072
      max_output_tokens: 1024    # Optimized for safety classification responses
      max_file_size_mb: 20       # Supports multimodal content (text + images)
      supports_vision: true      # Multimodal safety model
      
  # Groq - Llama Prompt Guard 2 22M (Preview)
  - model_name: Groq-Llama-Prompt-Guard-2-22M
    litellm_params:
      model: groq/meta-llama/llama-prompt-guard-2-22m
      api_base: https://api.groq.com/openai/v1
      api_key: os.environ/GROQ_API_KEY
      rpm: 30                    # Free Tier: 30 RPM, 14.4K RPD
      tpm: 15000                 # Free Tier: 15K TPM, 500K TPD
      # max_tokens: 512            # Specialized for prompt injection detection
      input_cost_per_token: 0.00000002  # Very low cost for lightweight model
      output_cost_per_token: 0.00000002
    model_info:
      mode: chat
      max_tokens: 512
      max_output_tokens: 512     # Designed for binary classification

  # Groq - Llama Prompt Guard 2 86M (Preview)
  - model_name: Groq-Llama-Prompt-Guard-2-86M
    litellm_params:
      model: groq/meta-llama/llama-prompt-guard-2-86m
      api_base: https://api.groq.com/openai/v1
      api_key: os.environ/GROQ_API_KEY
      rpm: 30                    # Free Tier: 30 RPM, 14.4K RPD
      tpm: 15000                 # Free Tier: 15K TPM, 500K TPD
      # max_tokens: 512            # Specialized for prompt injection detection
      input_cost_per_token: 0.00000005  # Low cost for lightweight model
      output_cost_per_token: 0.00000005
    model_info:
      mode: chat
      max_tokens: 512
      max_output_tokens: 512     # Designed for binary classification
      
  # Note: The following models (GPT-OSS, Compound) are not on the public Groq list.
  # Their parameters are based on the user's config and may require a special API tier.

  # Groq - GPT OSS 20B (OpenAI)
  - model_name: Groq-GPT-OSS-20B
    litellm_params:
      model: groq/openai/gpt-oss-20b
      api_base: https://api.groq.com/openai/v1
      api_key: os.environ/GROQ_API_KEY
      rpm: 30                    # Free Tier: 30 RPM, 1K RPD
      tpm: 8000                  # Free Tier: 8K TPM, 200K TPD
      # max_tokens: 128000
      input_cost_per_token: 0.0000001   # $0.10 per 1M input tokens
      output_cost_per_token: 0.0000005  # $0.50 per 1M output tokens
    model_info:
      mode: chat
      max_tokens: 128000

  # Groq - GPT OSS 120B (OpenAI)
  - model_name: Groq-GPT-OSS-120B
    litellm_params:
      model: groq/openai/gpt-oss-120b
      api_base: https://api.groq.com/openai/v1
      api_key: os.environ/GROQ_API_KEY
      rpm: 30                    # Free Tier: 30 RPM, 1K RPD
      tpm: 8000                  # Free Tier: 8K TPM, 200K TPD
      # max_tokens: 128000
      input_cost_per_token: 0.00000015  # $0.15 per 1M input tokens
      output_cost_per_token: 0.00000075 # $0.75 per 1M output tokens
    model_info:
      mode: chat
      max_tokens: 128000

  # Groq - Compound Beta Agentic System
  - model_name: Groq-Compound-Beta-Agentic-System
    litellm_params:
      model: groq/compound-beta
      api_base: https://api.groq.com/openai/v1
      api_key: os.environ/GROQ_API_KEY
      rpm: 15
      tpm: 70000
      # max_tokens: 128000
      # Note: Tool calls are free during beta
      input_cost_per_token: 0
      output_cost_per_token: 0
    model_info:
      mode: chat
      max_tokens: 128000
      supports_function_calling: true

  # Groq - Compound Beta Mini Agentic System
  - model_name: Groq-Compound-Beta-Mini-Agentic-System
    litellm_params:
      model: groq/compound-beta-mini
      api_base: https://api.groq.com/openai/v1
      api_key: os.environ/GROQ_API_KEY
      rpm: 15
      tpm: 70000
      # max_tokens: 128000
      # Note: Tool calls are free during beta
      input_cost_per_token: 0
      output_cost_per_token: 0
    model_info:
      mode: chat
      max_tokens: 128000
      supports_function_calling: true

  # Groq - Compound Agentic System (Production)
  - model_name: Groq-Compound-Agentic-System
    litellm_params:
      model: groq/compound
      api_base: https://api.groq.com/openai/v1
      api_key: os.environ/GROQ_API_KEY
      rpm: 30                    # Free Tier: 30 RPM, 250 RPD
      tpm: 70000                 # Free Tier: 70K TPM, No daily limit
      # max_tokens: 131072
      input_cost_per_token: 0.00000027  # Estimated pricing for production compound model
      output_cost_per_token: 0.00000027
    model_info:
      mode: chat
      max_tokens: 131072
      supports_function_calling: true

  # Groq - Compound Mini Agentic System (Production)
  - model_name: Groq-Compound-Mini-Agentic-System
    litellm_params:
      model: groq/compound-mini
      api_base: https://api.groq.com/openai/v1
      api_key: os.environ/GROQ_API_KEY
      rpm: 30                    # Free Tier: 30 RPM, 250 RPD
      tpm: 70000                 # Free Tier: 70K TPM, No daily limit
      # max_tokens: 131072
      input_cost_per_token: 0.00000015  # Estimated pricing for production compound mini model
      output_cost_per_token: 0.00000015
    model_info:
      mode: chat
      max_tokens: 131072
      supports_function_calling: true

  # ============================================================================
  # SPEECH-TO-TEXT (STT) / AUDIO TRANSCRIPTION MODELS
  # ============================================================================

  # Groq - Whisper Large V3 (Production)
  - model_name: Groq-Whisper-Large-V3
    litellm_params:
      model: groq/whisper-large-v3
      api_base: https://api.groq.com/openai/v1
      api_key: os.environ/GROQ_API_KEY
      rpm: 20                    # Free Tier: 20 RPM, 2K RPD
      # Audio limits: 7.2K seconds/hour, 28.8K seconds/day
      input_cost_per_hour: 0.111  # $0.111 per hour of audio
    model_info:
      mode: audio_transcription
      max_file_size_mb: 100

  # Groq - Whisper Large V3 Turbo (Production)
  - model_name: Groq-Whisper-Large-V3-Turbo
    litellm_params:
      model: groq/whisper-large-v3-turbo
      api_base: https://api.groq.com/openai/v1
      api_key: os.environ/GROQ_API_KEY
      rpm: 20                    # Free Tier: 20 RPM, 2K RPD
      # Audio limits: 7.2K seconds/hour, 28.8K seconds/day
      input_cost_per_hour: 0.04   # $0.04 per hour of audio
    model_info:
      mode: audio_transcription
      max_file_size_mb: 100

  # ============================================================================
  # TEXT-TO-SPEECH (TTS) MODELS
  # ============================================================================

  # Groq - PlayAI TTS (Preview)
  - model_name: Groq-PlayAI-TTS
    litellm_params:
      model: groq/playai-tts
      api_base: https://api.groq.com/openai/v1
      api_key: os.environ/GROQ_API_KEY
      rpm: 10                    # Free Tier: 10 RPM, 100 RPD
      tpm: 1200                  # Free Tier: 1.2K TPM, 3.6K TPD
      input_cost_per_character: 0.00005  # $50.00 per 1M characters
    model_info:
      mode: text_to_speech
      max_tokens: 8192

# The rest of your configuration (litellm_settings, router_settings, etc.) follow here.

# Global settings for LiteLLM proxy
litellm_settings:
  # Global rate limiting and budget settings
  max_budget: 1000.0              # Maximum budget in USD
  budget_duration: "30d"          # Budget reset period
  
  # Default fallback settings
  default_# max_tokens: 4096
  default_temperature: 0.7
  
  # Request timeout settings
  request_timeout: 600            # 10 minutes timeout
  
  # Retry settings
  num_retries: 3
  
  # Enable cost tracking
  cost_tracking: true
  
  # Enable caching for better performance
  redis_url: os.environ/REDIS_URL # Optional Redis for caching
  
  # Audio model settings
  max_file_size_mb: 100           # Global max file size for audio models
  supported_audio_formats: ["mp3", "mp4", "mpeg", "mpga", "m4a", "wav", "webm"]

# Router settings for load balancing
router_settings:
  routing_strategy: "least-busy"  # Options: simple-shuffle, least-busy, usage-based-routing
  
  # Cooldown settings for failed deployments
  cooldown_time: 30               # seconds
  
  # Health check settings
  health_check_interval: 300      # 5 minutes

# General settings
general_settings:
  # Master key for proxy authentication
  master_key: os.environ/LITELLM_MASTER_KEY
  
  # Database for storing usage data
  database_url: os.environ/DATABASE_URL
  
  # Enable UI dashboard
  ui: true
  
  # Enable detailed logging
  set_verbose: true