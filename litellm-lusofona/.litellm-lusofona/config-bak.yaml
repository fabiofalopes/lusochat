model_list:
  - model_name: DeepSeek-R1-Distill-Llama-8B-Q6_K
    litellm_params:
      model: "openai/deepseek-r1-llama-8b"  # openai/ prefix required for OpenAI-compatible endpoints
      api_base: "http://192.168.108.161:8080"
      api_key: "none"  # Assuming no auth required
      # max_tokens: 4096
    model_info:
      model_alias: "DeepSeek-R1-Distill-Llama-8B-Q6_K"
      host_node: "192.168.108.161"

  - model_name: DeepSeek-R1-Distill-Qwen-32B-Q4_K_M
    litellm_params:
      model: "openai/deepseek-qwen-32b"
      api_base: "http://192.168.108.164:8080" 
      api_key: "none"
      # max_tokens: 8192
    model_info:
      model_alias: "DeepSeek-R1-Distill-Qwen-32B-Q4_K_M"
      host_node: "192.168.108.164"

  - model_name: Mistral-Small-Instruct-2409-Q5_K_L
    litellm_params:
      model: "openai/mistral-small-instruct"
      api_base: "http://192.168.108.165:8080"
      api_key: "none"
      # temperature: 0.7
    model_info:
      model_alias: "Mistral-Small-Instruct-2409-Q5_K_L"
      host_node: "192.168.108.165"

  - model_name: Janus-Pro-7B-LM-Q6_K 
    litellm_params:
      model: "openai/janus-pro-7b"
      api_base: "http://192.168.108.167:8080"
      api_key: "none"
      # top_p: 0.9
    model_info:
      model_alias: "Janus-Pro-7B-LM-Q6_K"
      host_node: "192.168.108.167"

litellm_settings:
  callbacks: ["prometheus"]
  service_callback: ["prometheus_system"]
